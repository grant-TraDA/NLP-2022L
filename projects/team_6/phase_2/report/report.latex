    \documentclass[a4paper]{article}
\usepackage{blindtext}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[unicode]{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{outlines}
\usepackage{pdfpages}
\usepackage{geometry}
\usepackage{svg}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{titlesec}
\usepackage{float}
\usepackage{enumitem}
\usepackage{array}
\usepackage{color, colortbl}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{biblatex}
\addbibresource{p.bib}


\newcommand*{\naturalNumbers}{\mathbb{N}}%

\title{Hate speech classification}
\author{Paweł Flis, Karol Krupa, Rafał Chabasiński}
\date{April 2022}

\begin{document}
\maketitle

\section{Introduction}
Social media platforms such as Facebook, Twitter, Instagram and YouTube are experiencing a continuous growth in number of users. In recent years, partially due to COVID-19 pandemic even older generations turned to those platforms as a way to experience wider range of human interactions. Along with all the advantages social media brings, there are number of hazards related to harmful actions of some of its users encouraged by their perceived anonymity.
\par
Especially recently hate speech has been under the scope of investigation to make the internet a safe and welcoming space for everyone. This topic garnered scientific attention due to the potential of applying machine learning for hate speech recognition, which would in turn help counteract cyberbullying. Although the very idea of using automatic censors is highly questionable their counselling can be of invaluable aid. In this case the hate speech detector accuracy is of the essence as too rigorous model could severely affect freedom of expression while high-tolerance one would maintain unsafe environments.
\section{Literature analysis}
Number of Natural Language Processing in conjunction with Deep Learning solutions have been proposed during the years, however majority of these solutions requires great amount of labelled data, which has to be manually labeled by a human, which eventually which limits the cognitive capabilities of the model. It is also really hard to identify semantic nuances which often decide on the overall sentiment. There also have been attempts to tackle the problem using statistical models such as n-grams, SVM or Logistic Regression. These methods require vector as an input, so the corpus of text has to be earlier converted - this is commonly done using methods such as word2vec encoder using semantic similarity of words or doc2vec for entire sentences.
\par
The task at hand is hate speech detection in polish twitter data. Direct solution to the same problem were presented during PolEval2019~\cite{poleval}. At a time statistical models such as SVM with tokenization or even Deep Learning solutions such as LSTM were able to achieve only minor improvements over baseline classifier which always predicts most common class.
\par
Bidirectional Encoder Representations from Transformers model has shown a great prowess in various NLP tasks, in many cases outperforming its predecessors.
Mozafari et al.~\cite{paper} proposed a semi-supervised solution based on pre-trained BERT model with broad contextual knowledge and using transfer learning to fine tune a model on labelled samples from target domain of hate speech. 
\par
This approach considers two variants of BERT networks. $BERT_{base}$ having 12 layers and 110 million parameters and $BERT_{large}$ hacing double the layers and triple the parameters. The output is a sentence representation in a 768-dimensional vector, which is an input to domain specific classifier.
\par
In paper number of fine-tuning approaches are presented:
\begin{enumerate}
    \item \textbf{BERT based fine-tuning} \\
    $BERT_{base}$ model with one fully connected layer at the top and softmax activation function with no hidden layers.
    \item \textbf{Insert nonlinear layers} \\
    $BERT_{base}$ model with multilayer perceptron with two hidden layers at the top level.
    \item \textbf{Insert Bi-LSTM layer} \\
    $BERT_{base}$ model with Bi-LSTM recurrent neural network and one fully connected layer on the output.
    \item \textbf{Insert CNN layer} \\
    Output of each transformer from $BERT_{base}$ is concatenated to produce a matrix that is an input to CNN, later max pooling is performed and output is deduced using one fully connected layer once agian.
\end{enumerate}
\par
The authors performed data pre-processing to make the text input data more suitable for BERT model - emoticons were changed into <tags>, multiple word hashtags were separated and everything has been turned into lowercase. It is important to notice, that by doing that some relevant information may be lost, however we trust that it is a calculated decision and it is of benefit to used BERT model.
\par
Version with CNN layer proved to perform the best across all metrics (Precision, Recalll, F1-score). It outperformed other solutions designed for the very same datasets used in  ~\cite{ref1}~\cite{ref2}~\cite{ref3}. It is worth noting that BERT with nonlinear layers performed surprisingly poorly(much worse than with no hidded layers) on dataset with samples labelled as Hate[1430] Offensive[19190] Neither[4163] so dataset heavily biased towards 'just' offensive content.

\newpage
\section{Exploratory data analysis}
\subsection{Data examples}
\begin{itemize}
    \item non-harmful
        \begin{itemize}
            \item anonymizedaccount anonymizedaccount Zaległości były, ale ważne czy były wezwania do zapłaty z których się klub nie wywiązał.
            \item anonymizedaccount Tym w sejmie też? Banda hipokrytów i złodziei.
        \end{itemize}
    \item cyberbullying
        \begin{itemize}
            \item anonymizedaccount anonymizedaccount Czas pokaże kto jest kim, mącicielu, szkodniku, haha do gazu haha
            \item anonymizedaccount anonymizedaccount anonymizedaccount Pis mial być inny, jesteś idiotą
        \end{itemize}
    \item hate-speech
        \begin{itemize}
            \item RT anonymizedaccount anonymizedaccount Pasujecie do siebie antypolskie kanalie.
            \item anonymizedaccount Pierwszy priorytet zadowolić Jarkacza, drugi priorytet ssać kasę wszystkimi otworami,  mają rozdwojenie jaźni
        \end{itemize}
\end{itemize}

We can see that subjects are anonymized and are easily recognizable, which will be helpful for our algorithms.

\newpage
\subsection{Label data split}
\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/tags_bar_plot.png}
\caption{Labels distribution.}
\label{fig:labels_distribution}
\end{figure}

In \ref{fig:labels_distribution} we can see that distribution of classes in the dataset is very unbalanced. We'll have to consider that when creating our algorithm, by e.g.
\begin{itemize}
    \item shrinking bigger data sets to the smallest one
    \item applying bigger weights to smaller datasets
\end{itemize}


\newpage
\subsection{Data attributes}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/tweet_lengths_plot.png}
\caption{Tweets lengths (i.e number of characters in a tweet).}
\label{fig:tweet_lengths_histogram}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/tweet_word_counts_plot.png}
\caption{Tweets word counts.}
\label{fig:word_counts_histogram}
\end{figure}

In both \ref{fig:tweet_lengths_histogram} and \ref{fig:word_counts_histogram} we can see some outliers in the data. In order to simplify our task we should consider removing them.


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/tweet_word_counts_box_plot.png}
\caption{Tweets word counts box plot for different labels.}
\label{fig:box_plot}
\end{figure}

In \ref{fig:box_plot} different labels have quite similiar expected values of number of words, so this statistic probably won't help us.

\subsection{Word cloud}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{plots/word_cloud.png}
\caption{Word cloud.}
\label{fig:word_cloud}
\end{figure}

In \ref{fig:word_cloud} we can see that the most important topics in out dataset are football and politics, there are also some curse words which may help distinguish hate speech.

\newpage
\section{Solution concept}

Preparing our solution to the problem is to prepare a program that, given an expression written by a human, returns a result of 1 if the statement is hate-filled and 0 otherwise. 

In order to perform this task as well as possible, we need to:
\begin{enumerate}
    \item Properly transfer expressions and sentences into a form understandable by a computer - that is, vectorize them using an appropriate tokenizer. 
    \item Model the language and its internal dependencies using certain language models.
    \item Train the language models for our task, which is hate speech recognition.
\end{enumerate}

The first two steps can be solved by choosing a pretrained model. A collection of such models is available at this link \href{https://klejbenchmark.com/leaderboard/}{KLEJ-Benchmark}.
The first two steps are strongly related, since the trained model has been trained for selected embeddings and is unlikely to work for others.

An example model from the list above is HerBERT \cite{mroczkowski-etal-2021-herbert} and XLM-RoBERTa \cite{dadas2020pretraining}. Both score very high on the CBD task \cite{ptaszynski2019results}, which is the task we solve. For this reason, we decided to select pre-trained models and then retrain them for our purposes.
In addition, for the second model, the authors provided scripts that give the ability to train the model to a specific KLEJ-benchmark problem, including the CBD problem. The scripts are available here: \href{https://github.com/sdadas/polish-roberta}{scripts}.

The computations will be done with google cloud graphics cards, on the google collab site. Verification will be done using Precision, Recalll, F1-score metrics.

\newpage
\section{Solution}

\subsection{Data preprocessing}

During the development of the solution, a significant element was the proper preprocessing of the data. The proposed solution performs three-stage data processing:

\begin{enumerate}
    \item Optional stemming of words in sentences using \href{https://github.com/Tutanchamon/pl_stemmer/blob/master/pl_stemmer.py}{stemmer}.
    \item Main sentence preprocessing module consisting of:
    \begin{enumerate}
        \item Lowering letters
        \item Unidecoding
        \item Removing @anonymized\_account phrase, dots and commas
    \end{enumerate}
    \item Removing stopwords
\end{enumerate}

The reprocessed datasets are stored in the solution repository in folder data in files: test\_preprocessed.tsv, train\_preprocessed.tsv, test\_preprocessed\_stem.tsv, train\_preprocessed\_stem.tsv.

\subsection{Decision tree}

At the beginning of the preparation of the machine learning solution to the problem posed, we started the tests by building a model closest to the rule-based system. The choice was to build a decision tree based on CountVectorizer with 1 element ngrams. We ran a series of tests in which we examined the effect of tree depth on the results obtained. Results can be seen in figure \ref{fig:decision_tree_height_comparison}. 

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/decision_tree_height_comparison.png}
\caption{Decision tree height comparison}
\label{fig:decision_tree_height_comparison}
\end{figure}

Best obtained result for stemmed and not stemmed data are grouped in the table \ref{tab:decision_tree_hc_results}.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
No. & Stemmed data & Best tree height & F1 macro & F1 micro \\ \hline
1   & true         & 280              & 0.473    & 0.868    \\ \hline
2   & false        & 500              & 0.483    & 0.87     \\ \hline
\end{tabular}
\caption{Decision tree height comparison, results}
\label{tab:decision_tree_hc_results}
\end{table}

The performance of the decision trees increases during the initial phase of increasing the tree height, however, the fluctuations are quite high thereafter. This does not change the fact that good results have been achieved with the decision tree. However, it should be kept in mind that this solution is very dependent on the test set and does not take into account the meaning of the words, which can have very negative consequences when the vocabulary changes 


As part of the development of the solution, we have also prepared an illustration showing how the decision making is done for an example tree of height 3. The result is available in figure \ref{fig:example_tree}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/decision_tree_plot.png}
\caption{Example decision tree with depth = 3 and F1 macro 0.309}
\label{fig:example_tree}
\end{figure}


\subsection{SVM and Naive Bayes}

During the development of the solution, we experimented also with SVM models with three different kernels: RBF, Poly, Linear. In addition, we also considered the Naive Bayes model. To analyze these solutions, we proposed a text vectorizer computed from 1 or 2 element ngrams. Then the 5 or 10\% most significant elements were selected. Moreover, each experiment was performed twice - for the set with and without the applied stemmer. The collected results can be found in the table \ref{tab:simple_results}.


\begin{table}[h]
\centering
\begin{tabular}{llllllll}
\hline
\multicolumn{1}{|l|}{No.} & \multicolumn{1}{l|}{model}       & \multicolumn{1}{l|}{ngram} & \multicolumn{1}{l|}{selector\_percentile} & \multicolumn{1}{l|}{F1 macro (S)} & \multicolumn{1}{l|}{F1 micro (S)} & \multicolumn{1}{l|}{F1 macro} & \multicolumn{1}{l|}{F1 micro} \\ \hline
\multicolumn{1}{|l|}{1}   & \multicolumn{1}{l|}{SVM RBF}     & \multicolumn{1}{l|}{1}     & \multicolumn{1}{l|}{5}                    & \multicolumn{1}{l|}{0.365}        & \multicolumn{1}{l|}{0.873}        & \multicolumn{1}{l|}{0.350}    & \multicolumn{1}{l|}{0.871}    \\ \hline
\multicolumn{1}{|l|}{2}   & \multicolumn{1}{l|}{SVM RBF}     & \multicolumn{1}{l|}{1}     & \multicolumn{1}{l|}{10}                   & \multicolumn{1}{l|}{0.367}        & \multicolumn{1}{l|}{0.864}        & \multicolumn{1}{l|}{0.369}    & \multicolumn{1}{l|}{0.861}    \\ \hline
\multicolumn{1}{|l|}{3}   & \multicolumn{1}{l|}{SVM RBF}     & \multicolumn{1}{l|}{2}     & \multicolumn{1}{l|}{5}                    & \multicolumn{1}{l|}{0.399}        & \multicolumn{1}{l|}{0.869}        & \multicolumn{1}{l|}{0.403}    & \multicolumn{1}{l|}{0.871}    \\ \hline
\multicolumn{1}{|l|}{4}   & \multicolumn{1}{l|}{SVM RBF}     & \multicolumn{1}{l|}{2}     & \multicolumn{1}{l|}{10}                   & \multicolumn{1}{l|}{0.351}        & \multicolumn{1}{l|}{0.864}        & \multicolumn{1}{l|}{0.356}    & \multicolumn{1}{l|}{0.866}    \\ \hline
\multicolumn{1}{|l|}{5}   & \multicolumn{1}{l|}{SVM Poly}    & \multicolumn{1}{l|}{1}     & \multicolumn{1}{l|}{5}                    & \multicolumn{1}{l|}{0.398}        & \multicolumn{1}{l|}{0.875}        & \multicolumn{1}{l|}{0.366}    & \multicolumn{1}{l|}{0.873}    \\ \hline
\multicolumn{1}{|l|}{6}   & \multicolumn{1}{l|}{SVM Poly}    & \multicolumn{1}{l|}{1}     & \multicolumn{1}{l|}{10}                   & \multicolumn{1}{l|}{0.322}        & \multicolumn{1}{l|}{0.867}        & \multicolumn{1}{l|}{0.321}    & \multicolumn{1}{l|}{0.867}    \\ \hline
\multicolumn{1}{|l|}{7}   & \multicolumn{1}{l|}{SVM Poly}    & \multicolumn{1}{l|}{2}     & \multicolumn{1}{l|}{5}                    & \multicolumn{1}{l|}{0.339}        & \multicolumn{1}{l|}{0.87}         & \multicolumn{1}{l|}{0.339}    & \multicolumn{1}{l|}{0.869}    \\ \hline
\multicolumn{1}{|l|}{8}   & \multicolumn{1}{l|}{SVM Poly}    & \multicolumn{1}{l|}{2}     & \multicolumn{1}{l|}{10}                   & \multicolumn{1}{l|}{0.322}        & \multicolumn{1}{l|}{0.868}        & \multicolumn{1}{l|}{0.322}    & \multicolumn{1}{l|}{0.868}    \\ \hline
\multicolumn{1}{|l|}{\textbf{9}}   & \multicolumn{1}{l|}{\textbf{SVM Linear}}  & \multicolumn{1}{l|}{\textbf{1}}     & \multicolumn{1}{l|}{\textbf{5}}                    & \multicolumn{1}{l|}{\textbf{0.409}}        & \multicolumn{1}{l|}{\textbf{0.877}}        & \multicolumn{1}{l|}{\textbf{0.375}}    & \multicolumn{1}{l|}{\textbf{0.874}}    \\ \hline
\multicolumn{1}{|l|}{10}  & \multicolumn{1}{l|}{SVM Linear}  & \multicolumn{1}{l|}{1}     & \multicolumn{1}{l|}{10}                   & \multicolumn{1}{l|}{0.379}        & \multicolumn{1}{l|}{0.871}        & \multicolumn{1}{l|}{0.357}    & \multicolumn{1}{l|}{0.868}    \\ \hline
\multicolumn{1}{|l|}{11}  & \multicolumn{1}{l|}{SVM Linear}  & \multicolumn{1}{l|}{2}     & \multicolumn{1}{l|}{5}                    & \multicolumn{1}{l|}{0.389}        & \multicolumn{1}{l|}{0.872}        & \multicolumn{1}{l|}{0.389}    & \multicolumn{1}{l|}{0.872}    \\ \hline
\multicolumn{1}{|l|}{12}  & \multicolumn{1}{l|}{SVM Linear}  & \multicolumn{1}{l|}{2}     & \multicolumn{1}{l|}{10}                   & \multicolumn{1}{l|}{0.353}        & \multicolumn{1}{l|}{0.869}        & \multicolumn{1}{l|}{0.365}    & \multicolumn{1}{l|}{0.871}    \\ \hline
\multicolumn{1}{|l|}{13}  & \multicolumn{1}{l|}{Naive Bayes} & \multicolumn{1}{l|}{1}     & \multicolumn{1}{l|}{5}                    & \multicolumn{1}{l|}{0.322}        & \multicolumn{1}{l|}{0.868}        & \multicolumn{1}{l|}{0.322}    & \multicolumn{1}{l|}{0.868}    \\ \hline
\multicolumn{1}{|l|}{14}  & \multicolumn{1}{l|}{Naive Bayes} & \multicolumn{1}{l|}{1}     & \multicolumn{1}{l|}{10}                   & \multicolumn{1}{l|}{0.344}        & \multicolumn{1}{l|}{0.87}         & \multicolumn{1}{l|}{0.349}    & \multicolumn{1}{l|}{0.87}     \\ \hline
\multicolumn{1}{|l|}{15}  & \multicolumn{1}{l|}{Naive Bayes} & \multicolumn{1}{l|}{2}     & \multicolumn{1}{l|}{5}                    & \multicolumn{1}{l|}{0.322}        & \multicolumn{1}{l|}{0.868}        & \multicolumn{1}{l|}{0.322}    & \multicolumn{1}{l|}{0.868}    \\ \hline
\multicolumn{1}{|l|}{16}  & \multicolumn{1}{l|}{Naive Bayes} & \multicolumn{1}{l|}{2}     & \multicolumn{1}{l|}{10}                   & \multicolumn{1}{l|}{0.364}        & \multicolumn{1}{l|}{0.87}         & \multicolumn{1}{l|}{0.364}    & \multicolumn{1}{l|}{0.87}     \\ \hline
                          &                                  &                            &                                           &                                   &                                   &                               &                              
\end{tabular}
\caption{Different SVM and Naive Bayes settings, results, S - stemmed data}
\label{tab:simple_results}
\end{table}

It turns out that Linear SVM using 1 element grams and a 5\% selector on the data with the stemmer applied, turned out to be the best solution from this set. In addition, it can be inferred from the table that only in five cases we obtained an improvement in performance after using stemmer. These are the rows 1, 5, 6, 9, 10.


\subsection{Multilayer perceptron}

The next stage in the development of our solution was to prepare an MLP network with a structure consisting of the following parts:

\begin{itemize}
    \item Linear layer of size input\_size x 200
    \item Dropout layer 0.2
    \item ReLU
    \item Linear layer of size 200 x 50
    \item Dropout layer 0.2
    \item ReLU
    \item Linear layer of size 50 x 3
    \item Softmax
\end{itemize}


During the development of the solution, we prepared an environment to train MLPs along with various vectorizers. We tested the following text vectors as part of our testing:


\begin{itemize}
    \item CountVectorizer with ngrams of size 1, with selector 5\%
    \item CountVectorizer with ngrams of size 2, with selector 5\%
    \item TfidfVectorizer
    \item Spacy pl\_core\_news\_md vectorizer
    \item Spacy pl\_core\_news\_lg vectorizer
\end{itemize}

Each of these tests was performed on stemmed data and without applied stemmer. Each test was run with the Adam optimizer, with an initial learning rate of 1e-4, at 20 epochs, with the CrossEntropy loss function. Training data were reported with batches of size 10. We also implemented a custom subsampler that rejects 80\% of sentences with a label 0 (neutral sentence) - this operation was needed, otherwise the network learned to predict only one class. Also, in the beginning of every experiment random seed was set to same value (1).

The collected test results are shown in the table below.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
No. & Vectorizer                             & F1 macro (S) & F1 micro (S) & F1 macro & F1 micro \\ \hline
1   & CountVectorizer, ngram=1, percentile=5 & 0.472        & 0.874        & 0.485    & 0.876    \\ \hline
2   & CountVectorizer, ngram=2, percentile=5 & 0.441        & 0.874        & 0.427    & 0.877    \\ \hline
3   & \textbf{Tfidf}                                  & \textbf{0.481}        & \textbf{0.815}        & \textbf{0.500}    & \textbf{0.828}    \\ \hline
4   & Spacy, pl\_core\_news\_md              & 0.438        & 0.816        & 0.430    & 0.787    \\ \hline
5   & Spacy, pl\_core\_news\_lg              & 0.435        & 0.800        & 0.436    & 0.803    \\ \hline
\end{tabular}
\caption{MLP with different vectorizers, results. S - stemmed data, results for best F1 macro score epoch}
\label{tab:mlp_results}
\end{table}

The best solution turned out to be the one using the tfidf vectorizer on the data without the stemmer applied.

The figure \ref{fig:mlp_training} shows the training processes for each experiment.  

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{plots/mlp_with_different_embeddings_comparison.png}
\caption{MLP training processes}
\label{fig:mlp_training}
\end{figure}

One can see on the graph that the training process varies between methods up to about 5 epochs. After that, the process is similar, but the differences in F1 macro statistics are about a few percentage points.



\subsection{Deep learning}

To solve the task using deep learning methods, we used RoBERTa deep machine learning models along with scripts prepared to train them for the KLEJ benchmark prepared by Allegro. The available scripts are \href{https://github.com/sdadas/polish-roberta}{here}. 

During the development of the solution, we used the Collab Pro environment (Google's computing cloud) and the Tesla graphics card that was assigned to us there.

Solution preparation consisted of:
\begin{enumerate}
    \item Prepare the training and testing collection for the CBD task format from the scripts described above. Their Internet hate detection task involved distinguishing between neutral and hate speech. In our case, we have 3 classes: hate, cyber-bullying, and neutral. Moreover, the sets for this task appear in two files: tags and text. Obtaining the intended format required combining these files in tag tab text format.
    \item KLEJ benchmark script modification. The Task CBD described above had 2 categories and was not set up for post-training testing. Because of this, it required code modification. The modified tests.py file is available in the solution repository.
    \item Downloading models on Google Collab.
    \item Beginning model training.
    \item Generating labels for test data.
    \item Running evaluating \href{http://2019.poleval.pl/tasks/task6}{script}.
\end{enumerate}

We tested two models while conducting the experiments. The first one is RoBERTa-large-v2, a model trained on a corpus of about 200GB of data. When training this model, we used the following python script call:

python run\_tasks.py --arch roberta\_large --model\_dir roberta\_large --train-epochs 2 --tasks KLEJ-CBD --fp16 True --max-sentences 8 --update-freq 4 --resample 0:1,1:1,2:1

The training process took approximately 320 seconds (2 epochs) on a Tesla P4 16GB.
 
The parameters mean that we used the roberta\_large architecture, the model is in the roberta\_large folder, we want to train the model for 2 epochs, the task we are doing is TASK-CBD, we want to do the calculations with less precision (meaning they will be done faster), the maximum number of sentences in the GPU at one time is 8, the update rate is 4, the input resampling is 1 for class 0 (neutral), 1 for class 1 (cyber-bullying), 1 for class 2 (hate).

Next, we tested the samplings as 1:15:10 and 0.75:5:3. In addition, we tested the performance of the RoBERTa-base model trained on a 20GB body, which is 10x smaller than the RoBERTa-large-v2.

The results obtained can be found in \ref{tab:roberta_results}. 

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|ll}
\cline{1-3}
                 & micro-average F-score & macro-average F-score &  &  \\ \cline{1-3}
base-075-5-3     & 88.10                 & 57.24                 &  &  \\ \cline{1-3}
large-v2-075-5-3 & 90.00                 & 58.63                 &  &  \\ \cline{1-3}
large-v2-1-1-1   & 90.70                 & 51.48                 &  &  \\ \cline{1-3}
large-v2-1-15-10 & 89.50                 & 54.21                 &  &  \\ \cline{1-3}
\end{tabular}
\caption{RoBERTa with different subsampling methods, results}
\label{tab:roberta_results}
\end{table}

Accuracy was about 90\% in each case, however, it is not very reliable due to the huge advantage of the class of neutral statements.

The results obtained by the script authors were 66.77\% and 74.16\% for the base model and the large model, respectively. Note that the task they performed is significantly simpler, because they only had to distinguish between negative and neutral statements. In our case there is a neutral statement and a negative statement in two variants, which is a more difficult task.

We conducted an experiment using the RoBERT-large-v2 model along with 0:0.75, 1:3 resampling (used by the authors) for a class 2 CBD task. The results obtained are: precision 76.64\%, recall 61.19\%, balanced F-score 68.05\%, accuracy 92.3\%, so much better than in case of solution for 3 class problem.



\printbibliography

\end{document}
 