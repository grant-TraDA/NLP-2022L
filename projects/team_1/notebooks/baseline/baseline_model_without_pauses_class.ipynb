{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> Objectives\n",
    "The purpose of this notebook is creating a baseline rule-model, which is to be a benchmark of the neural model developed in the later phase of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:11:45.952948Z",
     "start_time": "2022-05-13T12:11:35.133423Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading packages\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))))\n",
    "from supportive_functions import rm_consecutive_spaces\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from polish_sentence_nltk_tokenizer import sentence_tokenizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "\n",
    "sns.set(\n",
    "    rc={\n",
    "        \"figure.figsize\": (14, 8.27),\n",
    "        \"axes.facecolor\": \"white\",\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.color\": \".9\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:11:45.969818Z",
     "start_time": "2022-05-13T12:11:45.954942Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_files(dir_path: str, sep: str = \",\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the files with timestamps from a given catalog.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): Path to the catalog with the files\n",
    "        sep (str, optional): Separator used in pd.read_csv() function. Defaults to \",\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data from all the files concatenated into one dataframe.\n",
    "    \"\"\"\n",
    "    files = os.listdir(dir_path)\n",
    "    data = pd.DataFrame(\n",
    "        [], columns=[\"index\", \"timestamp_start\", \"timestamp_stop\", \"word\"]\n",
    "    )\n",
    "    for file in tqdm(files):\n",
    "        f = open(os.path.join(dir_path, file), encoding=\"utf-8\", mode=\"r\")\n",
    "        name = file.split(\".\")[0]\n",
    "        df = pd.read_csv(f, header=None, sep=sep, encoding=\"utf-8\")\n",
    "        df[\"index\"] = name\n",
    "        if sep != \",\":\n",
    "            df[\"timestamp_start\"] = df.iloc[:, 0].str.split(\",\").str[0].str[1:]\n",
    "            df[\"timestamp_stop\"] = (\n",
    "                df.iloc[:, 0].str.split(\",\").str[1].str.split(\"\\)\").str[0]\n",
    "            )\n",
    "            df[\"word\"] = df.iloc[:, 0].str.split(\"\\)\\s\").str[1]\n",
    "            df.drop([0], axis=1, inplace=True)\n",
    "            df.drop(df.tail(1).index, inplace=True)\n",
    "        else:\n",
    "            df[\"timestamp_start\"] = df.iloc[:, 0].str[1:]\n",
    "            df[\"timestamp_stop\"] = df.iloc[:, 1].str.split(\"\\) \\s\").str[0]\n",
    "            df[\"word\"] = df.iloc[:, 0].str.split(\"\\)\\s\").str[1]\n",
    "\n",
    "            df.drop([0, 1], axis=1, inplace=True)\n",
    "            df.drop(df.tail(1).index, inplace=True)\n",
    "        data = pd.concat([data, df])\n",
    "        f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T12:11:45.991837Z",
     "start_time": "2022-05-13T12:11:45.971812Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_pauses(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate new features describing pauses between two timestamps.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataframe with preprocessed timestamp\n",
    "            data separated into 'timestamp_start' and 'timestamp_stop' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Input dataframe with two new features added,\n",
    "            describing the pause between the word and after it.\n",
    "    \"\"\"\n",
    "    data.timestamp_start = data.timestamp_start.astype(int)\n",
    "    data.timestamp_stop = data.timestamp_stop.astype(int)\n",
    "    data_shifted = pd.concat(\n",
    "        [data, data.timestamp_start.shift(-1), data.timestamp_stop.shift()], axis=1\n",
    "    )\n",
    "    data_shifted.columns = [\n",
    "        \"index\",\n",
    "        \"timestamp_start\",\n",
    "        \"timestamp_stop\",\n",
    "        \"word\",\n",
    "        \"timestamp_start_lead\",\n",
    "        \"timestamp_stop_lag\",\n",
    "    ]\n",
    "    data_shifted[\"pause_before\"] = (\n",
    "        data_shifted.timestamp_start - data_shifted.timestamp_stop_lag\n",
    "    )\n",
    "    data_shifted[\"pause_after\"] = (\n",
    "        data_shifted.timestamp_start_lead - data_shifted.timestamp_stop\n",
    "    )\n",
    "    data_shifted.drop(\n",
    "        [\"timestamp_start_lead\", \"timestamp_stop_lag\"], axis=1, inplace=True\n",
    "    )\n",
    "    # to discuss\n",
    "    data_shifted.pause_before.fillna(data_shifted.timestamp_start, inplace=True)\n",
    "    data_shifted.pause_after.fillna(0, inplace=True)\n",
    "    data_shifted.reset_index(drop=True, inplace=True)\n",
    "    for i, row in data_shifted.iterrows():\n",
    "        if (i > 0) and (\n",
    "            data_shifted.loc[i, \"index\"] != data_shifted.loc[i - 1, \"index\"]\n",
    "        ):\n",
    "            data_shifted.loc[i, \"pause_before\"] = data_shifted.loc[i, \"timestamp_start\"]\n",
    "            data_shifted.loc[i - 1, \"pause_after\"] = 0\n",
    "\n",
    "    return data_shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family:Georgia'> Implementation of rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_rules():\n",
    "    \"\"\"\n",
    "    Implement the rules for baseline model.\n",
    "\n",
    "    Returns:\n",
    "        Consecutive rules for baseline model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # tokenizer for separation into sentences\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
    "\n",
    "    # extracting list of abbreviations\n",
    "    abbreviations = sentence_tokenizer._params.abbrev_types\n",
    "    abbreviations_with_period = [x for x in abbreviations if '.' in x]\n",
    "\n",
    "    # separating abbreviations with periods inside them\n",
    "    spaced_abbreviations_with_period = []\n",
    "    for abbr in abbreviations_with_period:\n",
    "        if \".\" in abbr:\n",
    "            spaced_abbreviations_with_period.append(abbr.replace(\".\", \" \"))\n",
    "\n",
    "    punctuation = list(sentence_tokenizer.PUNCTUATION)\n",
    "    punctuation.append('-')\n",
    "    punctuation.remove(';')\n",
    "\n",
    "    # reading dict generated from morfeusz (all items from train dataset)\n",
    "    morfeusz_train = pd.read_csv(\n",
    "        \"../../data/outputs/feature_engineering/morfeusz/items_dict.csv\"\n",
    "    )\n",
    "\n",
    "    # list of conjuctions from morfeusz\n",
    "    conj_comp_list = morfeusz_train[\n",
    "        morfeusz_train.grammatical_class.str.contains(\"comp|conj\")\n",
    "    ].item.tolist()\n",
    "\n",
    "    # all conjuctions below are from conj_comp_list\n",
    "    # not_conj - we do not put coma before them\n",
    "    # yes_conj - we do put coma before them\n",
    "\n",
    "    not_conj = [\n",
    "        \"a\",\n",
    "        \"albo\",\n",
    "        \"ali\",\n",
    "        \"ani\",\n",
    "        \"aniżeli\",\n",
    "        \"aż\",\n",
    "        \"bowiem\",\n",
    "        \"bylem\",\n",
    "        \"bym\",\n",
    "        \"byśmy\",\n",
    "        \"bądź\",\n",
    "        \"co\",\n",
    "        \"com\",\n",
    "        \"coś\",\n",
    "        \"czyli\",\n",
    "        \"czym\",\n",
    "        \"ewentualnie\",\n",
    "        \"gdybyś\",\n",
    "        \"gdybyśmy\",\n",
    "        \"i\",\n",
    "        \"inaczej\",\n",
    "        \"jednak\",\n",
    "        \"kiedyś\",\n",
    "        \"lub\",\n",
    "        \"miast\",\n",
    "        \"miasto\",\n",
    "        \"natomiast\",\n",
    "        \"ni\",\n",
    "        \"niby\",\n",
    "        \"niczym\",\n",
    "        \"nie\",\n",
    "        \"nim\",\n",
    "        \"niż\",\n",
    "        \"niżem\",\n",
    "        \"oraz\",\n",
    "        \"tedy\",\n",
    "        \"to\",\n",
    "        \"tom\",\n",
    "        \"toteż\",\n",
    "        \"tudzież\",\n",
    "        \"tylko\",\n",
    "        \"tym\",\n",
    "        \"tymczasem\",\n",
    "        \"zamiast\",\n",
    "        \"zarówno\",\n",
    "        \"zatem\",\n",
    "        \"zaś\",\n",
    "    ]\n",
    "    yes_conj = [\n",
    "        \"aby\",\n",
    "        \"abyśmy\",\n",
    "        \"acz\",\n",
    "        \"aczkolwiek\",\n",
    "        \"bo\",\n",
    "        \"by\",\n",
    "        \"chociaż\",\n",
    "        \"chociażby\",\n",
    "        \"choć\",\n",
    "        \"choćby\",\n",
    "        \"czy\",\n",
    "        \"dopóki\",\n",
    "        \"gdy\",\n",
    "        \"gdyby\",\n",
    "        \"gdyż\",\n",
    "        \"im\",\n",
    "        \"iż\",\n",
    "        \"jak\",\n",
    "        \"jakby\",\n",
    "        \"jako\",\n",
    "        \"jakoby\",\n",
    "        \"jeśli\",\n",
    "        \"jeżeli\",\n",
    "        \"kiedy\",\n",
    "        \"ledwo\",\n",
    "        \"ponieważ\",\n",
    "        \"póki\",\n",
    "        \"skoro\",\n",
    "        \"więc\",\n",
    "        \"zaledwie\",\n",
    "        \"zanim\",\n",
    "        \"że\",\n",
    "        \"żeby\",\n",
    "        \"żebyś\",\n",
    "        \"żeś\",\n",
    "        \"ale\",\n",
    "        \"lecz\",\n",
    "    ]\n",
    "\n",
    "    # list of który która które\n",
    "    ktory_list = morfeusz_train[morfeusz_train.item.str.startswith(\"któr\")][\n",
    "        ~morfeusz_train.item.str.contains(\"ś\")\n",
    "    ].item.tolist()\n",
    "    ktory_list.remove(\"którykolwiek\")\n",
    "    ktory_list.remove(\"którędy\")\n",
    "\n",
    "    #additional list of conjunctions with coma which were not in morfeusz\n",
    "    additional_list=['dlatego']\n",
    "\n",
    "    #merge lists of conjuctions\n",
    "    con_list_yes = yes_conj + ktory_list + additional_list\n",
    "\n",
    "    return abbreviations, con_list_yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbols_and_noisy_words(which_set):\n",
    "    # load list of symbols to replace\n",
    "    symbols_to_replace_infile = open(\n",
    "        \"../../data/outputs/eda/symbols_to_replace.txt\", \"r\", encoding=\"utf-8\"\n",
    "    )\n",
    "    symbols_to_replace = symbols_to_replace_infile.read().splitlines()\n",
    "    # load list of noisy words, i.e. words with letters from outside the Polish alphabet\n",
    "    noisy_words_infile = open(\n",
    "        \"../../data/outputs/eda/\"+which_set+\"/noisy_words.txt\", \"r\", encoding=\"utf-8\"\n",
    "    )\n",
    "    noisy_words = noisy_words_infile.read().splitlines()\n",
    "    # load list of letters from outside the Polish alphabet\n",
    "    non_polish_letters_infile = open(\n",
    "        \"../../data/outputs/eda/\"+which_set+\"/non_polish_letters.txt\", \"r\", encoding=\"utf-8\"\n",
    "    )\n",
    "    non_polish_letters = non_polish_letters_infile.read().splitlines()\n",
    "    # merge noisy data into one list\n",
    "    noisy_word=copy.deepcopy(noisy_words)\n",
    "    noisy_word.extend(non_polish_letters)\n",
    "    # merge all data into one list\n",
    "    symbols_to_replace.extend(noisy_words)\n",
    "    symbols_to_replace.append('+')\n",
    "    symbols_to_replace.extend(non_polish_letters)\n",
    "    return symbols_to_replace, noisy_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel:\n",
    "    def __init__(self, which_set):\n",
    "        self.which_set = which_set\n",
    "        self.symbols_to_replace, self.noisy_word = symbols_and_noisy_words(which_set)\n",
    "        self.abbreviations, self.con_list_yes = implement_rules()\n",
    "        self.fullstop_threshold= 0.3 * 1000  # value based on the eda (0.3 seconds converted to miliseconds)\n",
    "    \n",
    "    def predict(self):\n",
    "        # load data\n",
    "        if self.which_set=='test':\n",
    "            data = read_files('../../data/source/poleval_fa.validation/validation', sep=\"\\t\")\n",
    "        elif self.which_set=='train':\n",
    "            data = read_files('../../data/source/poleval_fa.train/train', sep=\"\\t\")\n",
    "\n",
    "        data_calc=calculate_pauses(data).drop(\n",
    "            [\"timestamp_start\", \"timestamp_stop\"], axis=1\n",
    "        )\n",
    "\n",
    "        data_calc_joined = (data_calc[[\"index\", \"word\"]]\n",
    "        .groupby([\"index\"])[\"word\"]\n",
    "        .agg(\" \".join)\n",
    "        .reset_index()\n",
    "        )\n",
    "\n",
    "        #replacing symbols\n",
    "        if self.which_set=='train':\n",
    "            for symb in self.symbols_to_replace:\n",
    "                data_calc_joined[\"word\"] = data_calc_joined[\"word\"].apply(\n",
    "                    lambda x: x.replace(symb, \"\")\n",
    "                )\n",
    "\n",
    "            data_calc_joined[\"word\"] = data_calc_joined[\"word\"].apply(\n",
    "                rm_consecutive_spaces\n",
    "            )\n",
    "            data_calc_joined.columns = [\"FileId\", \"FixedOutput\"]\n",
    "            data = copy.deepcopy(data_calc_joined)\n",
    "\n",
    "        elif self.which_set=='test':\n",
    "            for symb in self.noisy_word:\n",
    "                data_calc_joined['word'] = data_calc_joined['word'\n",
    "                        ].apply(lambda x: x.replace(symb, ''))\n",
    "\n",
    "            data_calc_joined['word'] = data_calc_joined['word'\n",
    "                    ].apply(rm_consecutive_spaces)\n",
    "\n",
    "            for symb in self.symbols_to_replace:\n",
    "                data_calc_joined['word'] = data_calc_joined['word'\n",
    "                        ].apply(lambda x: x.replace(symb, ''))\n",
    "\n",
    "            data_calc_joined['word'] = data_calc_joined['word'\n",
    "                    ].apply(rm_consecutive_spaces)\n",
    "            data_calc_joined.columns = ['FileId', 'FixedOutput']\n",
    "            data = copy.deepcopy(data_calc_joined)            \n",
    "\n",
    "\n",
    "        # inserting fullstops after abbreviations + removing doubled spaces\n",
    "        for abbr in self.abbreviations:\n",
    "            data['FixedOutput'] = data['FixedOutput'\n",
    "                    ].apply(lambda x: x.replace(' ' + abbr + ' ', ' '\n",
    "                            + abbr.replace(' ', ' . ') + ' . '))\n",
    "        data['FixedOutput'] = data['FixedOutput'].apply(lambda x: \\\n",
    "                x.replace('. .', '.'))\n",
    "\n",
    "        # inserting commas according to polish language rules\n",
    "        for conj in self.con_list_yes:\n",
    "            data['FixedOutput'] = data['FixedOutput'\n",
    "                    ].apply(lambda x: x.replace(' ' + conj + ' ', ' , ' + conj\n",
    "                            + ' '))\n",
    "        data['FixedOutput'] = data['FixedOutput'].apply(lambda x: \\\n",
    "                x.replace('. ,', ','))\n",
    "\n",
    "        #saving output to file\n",
    "\n",
    "        # if self.which_set=='train':\n",
    "        #     data.to_csv(\n",
    "        #         '../../data/outputs/baseline/including_pauses/train_data.csv',\n",
    "        #         index=False\n",
    "        #     )\n",
    "        # elif self.which_set=='test':\n",
    "        #     data.to_csv(\n",
    "        #         \"../../data/outputs/baseline/including_pauses/test_data.csv\", index=False\n",
    "        #     )\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 793/793 [00:15<00:00, 52.26it/s]\n"
     ]
    }
   ],
   "source": [
    "output_train=BaselineModel('train').predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we wrocławiu walkę ze szkodnikiem rozpoczyna zespół szkół budowlanych przy ul . grabiszyńskiej do akcji na ratunek kasztanowcom zsb zachęcił pobliskie szkoły gimnazjum nr . 6 przy al . pracy sp . nr . 82 przy ul . blacharskiej sp . nr . 109 przy ul . inżynierskiej sp . nr . 15 przy ul . solskiego gimnazjum nr . 40 przy ul . morelowskiego pomoc ma polegać na szukaniu sponsorów , którzy sfinansują specjalistyczne szczepienia koszt szczepienia jednego drzewa to ok . 70 zł vat sponsor , który uratuje kasztanowca ma zapewnioną na nim tabliczkę reklamową podobna akcja odbyła się w świdnicy gdzie szczepionkę dla 99 drzew zasponsorowała firma imp comfort , która kupiła 10 ha . gruntów w mieście i zamierza wybudować tam swoją fabrykę szrotówek kasztanowcowiaczek już od jakiegoś czasu niszczy kasztanowce do niedawna uważano , że tego szkodnika można wytępić tylko poprzez palenie i kompostowanie liści kasztanowca teraz wiemy , że możemy je uratować poprzez specjalne szczepionki , jeśli możesz pomóc wrocławskim kasztanowcom to skontaktuj się z zsb ul . grabiszyńska 236 we wrocławiu telefon 363 25 29 konto bankowe kb iv o wrocław 71 1500 1793 1226 1007 1791 0000 z dopiskiem kasztanowiec '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train.loc[0, 'FixedOutput']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='font-family:Georgia'> Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 58.26it/s]\n"
     ]
    }
   ],
   "source": [
    "output_test=BaselineModel('test').predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prezydent usa george bush powiedział , że odnowa nowego orleanu i innych zalanych terenów potrwa lata stan luizjana wraz ze stolicą nowym orleanem ucierpiał w wyniku ataku huraganu katrina prezydent skrócił swoje wakacje w teksasie oraz zwołał posiedzenie gabinetu , który obejmie kontrolę nad nadzorem akcji ratunkowej jednej z najgorszych katastrof naturalnych bush wyznaczył trzy priorytety dotyczące walki ze skutkami kataklizmu ratowanie życia ofiarom w tym poszukiwanie zaginionych pomoc poszkodowanym i odnalezionym odbudowa i usuwanie szkód w ramach akcji ratunkowej przydzielono dodatkowo 10 tysięcy żołnierzy , którzy wspomogą operację w regionach najbardziej dotkniętych katastrofą zaliczają się do nich części stanów luizjany i mississippi służby medyczne zaoferowały ponad 10 tysięcy łóżek zaś armia amerykańska wysyła swoje helikoptery i łodzie , aby uratować mieszkańców znajdujących się w schronieniu późnym wieczorem w środę rozpoczęła się ewakuacja reszty mieszkańców nowego orleanu tych , którzy pozostali w schronach oraz na stadionie superdome w centrum miasta ich liczbę szacuje się na 20000 ludzie są wywożeni do oddalonego o 350 mil stadionu astrodome w houston w tym celu podstawiono 300 autobusów niektórzy wykorzystują sytuację i w poszukiwaniu jedzenia lub cennych rzeczy włamują się do domów biur szpitali rozbijają szyby wyłamują drzwi wleką za sobą to co udaje się , im . unieść bądź przenieść uderzenie huraganu katrina spowodowało wzrost cen benzyny w wielu częściach stanów zjednoczonych osiągnęła ona wartość powyżej 3 dolarów za galon w niektórych stanach wynosi ona 3 50 nad zatoką meksykańską umiejscowiony jest jeden z największych okręgów przemysłowych mieszczącym wiele rafinerii oraz rurociągów'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test.loc[1, 'FixedOutput']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1eea36f549d596e1ba6d357da17c58a2c496985ce50cff93b35e7c83413effd0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
